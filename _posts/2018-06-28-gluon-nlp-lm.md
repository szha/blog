---
title: GluonNLP V0.3 语言模型
author: 王晨光 Amazon Applied Scientist
---

## 语言模型篇

![](img/kaibuliaokou.png){:width="500px"}

当时年幼无知的你看到杰伦唱:“*<span style="color:gray">就是开不了口，让她知道</span>*”，心里的潜台词一定是这样的：

![](img/gewhat.jpg){:width="300px"}

就是：*<span style="color:blue">杰伦究竟为啥开不了口？</span>*不行我来，我们感到很生气（暴露了年龄orz）。现在做了GluonNLP的我终于明白，原来是杰伦的*语言模型*不行。如果您也有*类似症状*😀新版本GluonNLP V0.3提供了最先进的治疗方案，即目前市面上表现最佳的*Cache语言模型*，了解一下。

*<span style="color:orange">啥是语言模型?</span>*说人话的解释就是：让计算机模拟人能听懂、读懂的语言。*<span style="color:orange">那语言模型能干啥？</span>*基本上自然语言处理涉及到的领域都离不开语言模型，例如机器翻译，最近很火的问答系统，聊天机器人；还能有一些比较上档次的人工智能大新闻，比如常听到的人工智能写诗，写歌词啥的，其实背后的大佬就是语言模型。

于是患有类似症状的我，感觉再不学习语言模型就玩完了的我，几个月前上了船，GluonNLP几个小伙伴Sheng Zha ([@szha](https://github.com/szha) [@home](https://www.linkedin.com/in/shengzha))，Xingjian Shi ([@sxjscience](https://github.com/sxjscience) [@home](https://home.cse.ust.hk/~xshiab/))就跟我([@cgraywang](https://github.com/cgraywang) [@home](https://sites.google.com/site/raychenguangwang))造了各种语言模型。

这次新发布的Cache语言模型为啥用过的都说好？因为效果好，能把PPL从之前最优的 *<span style="color:orange">69.74 提升到 54.51</span>*（PPL是公认的语言模型评价指标，越低越好）。本质上Cache语言模型是*记性更好的深度循环神经网络*。要进一步解释Cache语言模型原理，我们就得言归正传一下，首先传统语言模型的正式定义是：给定一系列词，预测接下来的一个词。传统的语言模型是n元语言模型，简单的来说，就是计算给定n-1个词，预测第n个词的概率（没错~就是条件概率），然后把一个句子中所有的n-gram概率相乘，就是整个句子或者语言的概率。例如二元语言模型(bigram)，就是给定1个词，预测第2个词的概率，然后就能计算出某个句子的语言概率。Cache语言模型的基本假设是：如果一个词已经在一篇文章里面出现过了，那么这个词相对于其他词就更有可能出现在同一篇文章中。例如，*老虎*这个词在维基百科上*老虎*标题的页面中出现的频率是2.8%，而在整个维基百科文章中出现的频率则是0.0037%。Cache语言模型就是通过对语言中存在的词之间的长关联进行更好的建模，从而提升语言模型的效果。换句话说，Cache语言模型有一个叫做Cache的记忆单元，其中包含近期出现过的词（比如同一篇文章，或者某种宽度的窗口中）。简单从模型的实现细节上来讲，Cache存储了长短期记忆（LSTM）模型某段的隐状态（hidden states）与已经看到的词，然后将这段隐状态作为查询关键词，与当前隐状态进行简单的向量之间的内积运算，得到已经看到的词在Cache中的概率分布。该概率分布将直接作用到传统语言模型的概率分布上（例如上面提到的二元语言模型），从而提升语言模型的性能。Cache语言模型主要有三个好处：

* 可以帮助把已经在某个领域的数据上训练好的语言模型直接应用到新的领域；

* 能够更好的预测词典之外的词（OOV），因为只要是见到一次词典之外的词，Cache就能记住；

* 对于语言中的长关联现象有更好的把握。

写完上面这段打算收工提前下班了，路遇Sheng Zha小哥随即聊起，发现上面关于Cache语言模型虽然还算简单明了，但是容易被语言模型专家喷（orz）。所以这里要特别说明一下，Cache语言模型因为其特殊性，即需要把真实的近期出现过的词存储到Cache中，所以无法很直接的扩展到生成类的任务，例如，机器翻译或问答系统，因为生成类任务往往假设过去出现的真实的词是无法拿到了。若我们假设过去出现的真实的词是能够很容易拿到的，那么Cache语言模型也就能够直接用到生成类的任务中。否则，GluonNLP当然也提供的解决方案，因为GluonNLP中已经有了其他各种语言模型，例如基于循环神经网络（RNN）的语言模型，以及一些最新的其他功能强大的语言模型，例如，AWD语言模型。在这里做个预告，我们将会在后续的博客中针对语言模型跟大家进行更细致全面的分享~

所以，类似于水浒武松在跟咱们聊他打虎猛准狠的故事，Cache语言模型能够更好的记住武松刚提到的*老虎*，咱们也会接着跟武松聊*老虎*，而不是*老鼠*（==避免被武松大哥削）。

![](img/laohulaoshu.png){:width="500px"}

于是自从有了GluonNLP语言模型后的杰伦，不再开不了口，而是变身诗人（*LAOSIJI*）：

![](img/qinghuaci.png){:width="500px"}

想要了解更多的语言模型技术细节，请参考[这里](https://gluon-nlp.mxnet.io/scripts/index.html#language-model)和持续关注我们的公众号哦~
